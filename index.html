<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="TODO">
    <meta name="author" content="TODO">
    <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate">
    <meta http-equiv="Pragma" content="no-cache">
    <meta http-equiv="Expires" content="0">
    
    <title>Pitch-Conditioned Instrument Sound Synthesis from an Interactive Timbre Latent Space</title>
    
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <script src="js/audio_demo.js" defer></script>
</head>
<body>
    <div id="outer">
        <div id="inner">
            <section id="headsection">
                <div id="inner_headsection">
                    <h1 id="title">Pitch-Conditioned Instrument Sound Synthesis from an Interactive Timbre Latent Space</h1>
                    <h3 id="authors">Authors: Christian Limberg*, Fares Schulz*, Zhe Zhang, Stefan Weinzierl</h3>
                    <h2 id="abstract">Abstract</h2>
                    <p id="abstract_txt">
                        This paper presents a novel approach to instrument sound synthesis using a two-stage semi-supervised learning method capable of generating pitch-accurate, high-quality music samples from an expressive timbre latent space. Building upon our previous Generative Sample Map (GESAM) framework, which compresses the manifold of a drum sound dataset into a two-dimensional audio map, this method extends the representation of various musical instruments with pitch conditioning. In addition to the GESAM framework with a latent space learned via a Variational Autoencoder (VAE) and a Transformer model as the sound generator, we further propose a learning scheme that realizes the disentanglement of pitch and timbre information in the latent space. We demonstrate that the proposed method effectively learns a disentangled timbre latent space, enabling a more expressive and controllable generation process with effective pitch conditioning. The modelâ€™s performance is evaluated through a series of experiments, showing its ability to capture fine nuances in timbre while maintaining a high level of pitch accuracy. Finally, we showcase the usability of our approach through an interactive web application on this website.
                     </p>
                </div>
            </section>

            <section class="figure">
                <div class="inner_figure">
                    <h3 class="figure_title">Audio Sample Generator</h3>
                    <div id="slider_container">
                        <input type="range" id="size_slider" min="48" max="72" value="60">
                        <span id="slider_value">60</span>
                    </div>
                    <div id="selection_div">
                        <canvas id="selection_canvas"></canvas>
                        <svg viewBox="-1 -1 2 2" id="selection_svg"></svg>
                    </div>
                    <img src="data/generate_scatter/legend.svg" alt="Legend" class="legend-image" style="width:100%; margin-top: 20px;">
                    <p class="figure_caption">
                        Our interactive interface lets you generate musical samples by selecting points on a 2D plane. To select a pitch for generation, you have two options: 1. use the slider to select the desired note or 2. use your computer keyboard, which is mapped to a MIDI piano starting with note C3 on the 'a' key. The 'q' key allows you to toggle between two octaves. A fraction of the training data is displayed for orientation. Click on a certain location to play the related sample (speaker or headphones recommended).
                    </p>
                </div>
            </section>

            <section class="figure">
                <div class="inner_figure">
                    <h3 class="figure_title">Model Architecture</h3>
                    <img src="gfx/pgesam.svg" alt="Model Architecture" style="width:100%;">
                    <p class="figure_caption">
                        This schematic depicts the training procedure of our model. In the first stage, a VAE with a 2D latent bottleneck is trained. In the second stage, the Transformer model is trained with the VAE as a conditioning model.
                    </p>
                </div>
            </section>

            <section class="figure">
                <div class="inner_figure">
                    <h3 class="figure_title">Cite Us</h3>
                    <pre id="citebox">
@inproceedings{DAFx25_paper_58,
    author = "Limberg, Christian and Schulz, Fares and Zhang, Zhe and Weinzierl, Stefan",
    title = "{Pitch-Conditioned Instrument Sound Synthesis From an Interactive Timbre Latent Space}",
    booktitle = "Proceedings of the 28-th Int. Conf. on Digital Audio Effects (DAFx25)",
    editor = "Gabrielli, L. and Cecchi, S.",
    location = "Ancona, Italy",
    eventdate = "2025-09-02/2025-09-05",
    year = "2025",
    month = "Sept",
    publisher = "",
    issn = "2413-6689",
    doi = "",
    pages = ""
}
                    </pre>
                </div>
            </section>
        </div>
    </div>
</body>
</html>
